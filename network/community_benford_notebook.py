#%% [markdown]
# # Exploring Brazil Plutocracy Network

#%%

from pathlib import Path
import sys
import os
parent_dir = Path().cwd()
print((Path(parent_dir) / 'assets/'))
sys.path.append(str(Path(parent_dir) / 'assets/'))

import benford as bf
import generative_model
from adj_matrix import draw_adjacency_matrix, assignmentArray_to_lists

import pandas as pd
import networkx as nx
import seaborn as sns
import matplotlib
import numpy as np
import leidenalg as louvain

import itertools
import csv
from collections import defaultdict  



#%%
raw_data_path = Path(parent_dir) / 'raw_data/2014/'
data_path     = Path(parent_dir) / 'data/'
table_path    = Path(parent_dir) / 'tables/'
figure_path   = Path(parent_dir) / 'figures/'

directories = [table_path, figure_path]

for directory in directories:
    if not os.path.exists(directory):
        os.makedirs(directory)


#%%
import matplotlib.pyplot as plt
import matplotlib as mpl
mpl.use("pgf")
pgf_with_pdflatex = {
    "pgf.texsystem": "pdflatex",
    "pgf.preamble": [
         r"\usepackage[utf8x]{inputenc}",
         r"\usepackage[T1]{fontenc}",
         r"\usepackage{cmbright}",
         ]
}
mpl.rcParams.update(pgf_with_pdflatex)
#get_ipython().run_line_magic('matplotlib', 'inline')

from IPython.display import set_matplotlib_formats
set_matplotlib_formats('pdf', 'png')
pd.options.display.float_format = '{:.2f}'.format
rc={'savefig.dpi': 75, 'figure.autolayout': False, 'figure.figsize': [12, 8], 'axes.labelsize': 18,   'axes.titlesize': 18, 'font.size': 14, 'lines.linewidth': 2.0, 'lines.markersize': 8, 'legend.fontsize': 16,   'xtick.labelsize': 16, 'ytick.labelsize': 16}

sns.set(style='whitegrid',rc=rc)

#%% [markdown]
# # Introduction
#%% [markdown]
# Recent advances on technologies behind the Internet and mobile phone allowed bilions of people to conect and interact to each other in way never seen before. As a consequence, entirely new scales of data are generated daily, for example Facebook alone reports 1.32 billion daily active users on average for June 2017 \footnote{https://newsroom.fb.com/company-info/} and in 2014 reported about 600 TB daily rate of data \footnote{https://code.facebook.com/posts/229861827208629/scaling-the-facebook-data-warehouse-to-300-pb/}. The need for methodologies in order to extrat information (knowledge) from such data's volume and variety scaled at the same rate and scientific fields, like physics, whose experience of huge amounts of data generated by experiments like CERN that generate ~ 25GB of data per second or ~2 PB daily \footnote{https://home.cern/about/computing/processing-what-record} or Dark Energy Survey that report 2PB of data to analise \footnote{http://lss.fnal.gov/archive/2012/pub/fermilab-pub-12-930-a.pdf}, have a lot of methods at disposal. 
# 
# 
# Recently, several Brazil's representatives are facing accusations of receive campaign donations 
# in exchange of favors and/or to laundry money. 
# 
# A corruption scandal involving millions of dollars 
# in kickbacks and more than 80 politicians and members of the business elite. Odebrecht, 
# the Brazilian-based construction giant, which is Latin America's largest construction conglomerate, 
# has admitted bribing officials to secure contracts in Brazil and other countries in South America. 
# Marcelo Odebrecht, former CEO, says part of the $48m he donated to both Ms Rousseff's and Mr Temer's 
# campaigns in the 2014 Brazilian presidential election was illegal.
# 
# ###### CONETION BETWEEN DATA SCIENCE, STATISTICAL PHYSICS AND POLITICS HERE
# 
# 
# Since campaign donations play a vital rule for an election of a given candidate\cite{gammerman-antunes} 
# and can be made directly by persons or companies or indirectly by intermediary institutions, 
# such as electoral committees and other companies linked to political parties, Complex Network Theory can
# serve as a tool to analyze how the amounts received are distributed among the candidates. 
# 
# 
# 
# The Complex Network Theory has driven both qualitative and quantitative analysis of party-political networks, 
# encompassing investigations ranging from a description of politicians behavior in polls to the detection of communities that 
# arise from the pattern of bills co-sponsorship 
# \cite{DalMaso2014, Chessa2014, Bursztyn2015a, Zhang2008a, Waugh2009, Porter2005c, Porter2007a, Fowler2006, Waugh2009}.
# 
# In particular, community detection is one of the main tasks of Complex Network Theory and 
# such methods don't need incorporate any specific knowledge about committee members or 
# political parties, providing a suitable approach for a unbiased scientific investigation. 
#%% [markdown]
# # Data
#%% [markdown]
# FAZER ISSO PARA CADA COMUNIDADE
# 
# Inserir histograma de percentual de empreasas vs numero de candidatos favorecidos
# Fig. 1 shows histograms of the total number of bills sponsored by each legislator in each
# Congress for the House and Senate on a logarithmic plot. For comparability between the House
# and Senate the counts are converted to percent of chamber and pooled across Congresses, but
# the distributions for individual Congresses (not shown) tell the same story. These distributions
# are clearly not power law distributed. In contrast to the large number of scholars who publish
# five scientific papers or less, most legislators sponsor five bills or more (91% of legislators in the
# House and 99% of legislators in the Senate).
# 
# ----
# Numero de doadores por candidato
# Fig. 2 shows the distribution of the number of cosponsors per bill on a log–log plot. To aid
# in comparing the House and Senate, the number of cosponsors is divided by the total number of
# legislators in the chamber. Notice that the distributions for the House and Senate are quite close,
# suggesting that the cosponsorship process in both branches is similar. In fact, for bills cosponsored
# by up to 49% of the chamber, these distributions look like the power law distributions of number
# of coauthors per article found in the scientific authorship literature. A simple log–log regression
# of cosponsors as a percent of chamber on frequency of bills suggests a power law exponent of
# γ = 1.69 (S.E. 0.03, R2 = 0.94) in the House and γ = 1.59 (S.E. 0.04, R2 = 0.97) in the Senate
#%% [markdown]
# The network analyzed in this paper were assembled using data made available on-line by the TSE. For each federal election (from 2002 to 2016), the TSE provides two datasets for each federal unity and the country, containing electoral campaign spending and recipes. In this work we focus in donations made during electoral campaign of 2014, summarized above: 
#%% [markdown]
# From the data, we can have donations made in two possible ways:
# 
# 
# - Direct, where a company (characterized by a CNPJ number of 14 digits) or a person (characterized by a CPF of 11 digits) donate to a candidate;
# - Indirect, where a donation made by a company or a person are made to a committee or party representative legal institution (characterized by a CNPJ number of 14 digits) then reach the candidate.
# 
# 
# The fields of interest are \it{CPF candidato}, \it{CPF/CNPJ do doador} and \it{CPF/CNPJ do doador orinário}, the first representing the candidate, the second representing the intermediary legal entity when it exists or the donator if the donation are made directly, and the third representing the original donator when the intermediary are present. 
#%% [markdown]
# {'CNPJ Prestador Conta': 'id_accountant_cnpj',
#  'CPF do candidato': 'id_candidate_cpf',
#  'CPF/CNPJ do doador': 'id_donator_cpf_cnpj',
#  'CPF/CNPJ do doador originário': 'id_original_donator_cpf_cnpj',
#  'Cargo': 'cat_political_office',
#  'Cod setor econômico do doador': 'id_donator_economic_sector',
#  'Cód. Eleição': 'id_election',
#  'Data da receita': 'dat_donation_date',
#  'Data e hora': 'dat_donation_date_time',
#  'Desc. Eleição': 'cat_election_description',
#  'Descricao da receita': 'cat_donation_description',
#  'Especie recurso': 'cat_donation_method',
#  'Fonte recurso': 'cat_donation_source',
#  'Nome candidato': 'cat_candidate_name',
#  'Nome do doador': 'cat_donator_name',
#  'Nome do doador (Receita Federal)': 'cat_donator_name2',
#  'Nome do doador originário': 'cat_original_donator_name',
#  'Nome do doador originário (Receita Federal)': 'cat_original_donator_name2',
#  'Numero Recibo Eleitoral': 'id_receipt_num',
#  'Numero candidato': 'id_candidate_num',
#  'Numero do documento': 'id_document_num',
#  'Número candidato doador': 'id_donator_number',
#  'Número partido doador': 'id_donator_party',
#  'Sequencial Candidato': 'id_candidate_seq',
#  'Setor econômico do doador': 'cat_donator_economic_sector',
#  'Setor econômico do doador originário': 'cat_original_donator_economic_sector',
#  'Sigla  Partido': 'cat_party',
#  'Sigla UE doador': 'cat_donator_state',
#  'Tipo doador originário': 'cat_original_donator_type',
#  'Tipo receita': 'cat_donation_type',
#  'UF': 'cat_election_state',
#  'num_donation_ammount': 'num_donation_ammount',
#  'doador': 'id_donator_effective_cpf_cnpj'}

#%%
for role in roles:
    candidatos = pd.read_csv(data_path / f'brazil_2014_{role}_donations_candidates.csv')

    #%%
    n_candidatos    = candidatos['id_candidate_cpf'].nunique()
    n_doacoes       = candidatos.shape[0]
    tot_doacoes     = candidatos['num_donation_ammount'].sum()
    g_canditato     = candidatos.groupby('id_candidate_cpf').agg({'id_election': lambda x: len(x),'num_donation_ammount': lambda x: x.sum()  })
    g_especie       = candidatos.groupby('cat_donation_method').agg({'id_candidate_cpf': lambda x: x.nunique(),'id_election': lambda x: len(x),'num_donation_ammount': sum  })
    g_setor         = candidatos.groupby('cat_original_donator_economic_sector').agg({'id_candidate_cpf': lambda x: x.nunique(),'id_election': lambda x: len(x),'num_donation_ammount':lambda x: [x.sum(), x.mean(), x.std()] })
    g_uf            = candidatos.groupby('cat_election_state').agg({'id_candidate_cpf': lambda x: x.nunique(),'id_election': lambda x: len(x),'num_donation_ammount': sum  })
    g_partido       = candidatos.groupby('cat_party').agg({'id_candidate_cpf': lambda x: x.nunique(),'id_election': lambda x: len(x),'num_donation_ammount': sum  })
    g_setor_uf      = candidatos.groupby(['cat_original_donator_economic_sector','cat_election_state']).agg({'id_candidate_cpf': lambda x: x.nunique(),'id_election': lambda x: len(x),'num_donation_ammount':lambda x: [x.sum(), x.mean(), x.std()] })
    g_setor_partido = candidatos.groupby(['cat_party','cat_original_donator_economic_sector']).agg({'id_candidate_cpf': lambda x: x.nunique(),'id_election': lambda x: len(x),'num_donation_ammount': sum  })



    #%% [markdown]
    # The donations cover unniformily seven orders of magnitude
    #%% [markdown]
    # ## How different economic sectors donate? 

    #%%
    g_sector         = candidatos.groupby('cat_original_donator_economic_sector').agg({'id_candidate_cpf': lambda x: x.nunique(),'id_election': lambda x: len(x),'num_donation_ammount':lambda x: [x.sum(), x.mean(), x.std()] })
    tab_sector       = g_sector.sort_values(by='num_donation_ammount', ascending=False).reset_index()


    #%%
    tab_sector


    #%%
    tab_sector["Total Ammont"] = tab_sector['num_donation_ammount'].apply(lambda x: x[0])
    tab_sector["Mean Ammount"] = tab_sector['num_donation_ammount'].apply(lambda x: x[1])
    tab_sector["Std  Ammount "]  = tab_sector['num_donation_ammount'].apply(lambda x: x[2])
    tab_sector.drop('num_donation_ammount', axis=1, inplace= True)


    #%%
    tab_sector.columns = ['Economic Sector', 'Number of Candidates', 'Number of Donations', 'Total (R$)','Mean (R$)', 'Standard Deviation (R$)' ]


    #%%
    n_donations     = tab_sector['Number of Donations'].sum()
    total_donations = tab_sector['Total (R$)'].sum()


    #%%
    tab_sector[tab_sector['Number of Candidates'] > 0.01*n_candidatos]


    #%%
    print("Donation Statistics by Economic Sector:\n", tab_sector[tab_sector['Number of Candidates'] > 0.01*n_candidatos])
    with open(table_path / f'brazil_2014_{role}_statistics_by_sector.tex', 'w') as tf:
        tf.write(tab_sector[tab_sector['Number of Candidates'] > 0.01*n_candidatos].to_latex(index=False))


    #%%
    print('The 3 economic sectors that most donated to candidates are:')
    [print(i) for i in tab_sector['Economic Sector'][:3].values]


    #%%
    print('Alone, they are reponsable from', round(tab_sector['Number of Donations'][:3].sum()/n_donations*100,1), '% of all donations.')


    #%%
    print('Corresponding to',  round(tab_sector['Total (R$)'][:3].sum()/total_donations*100,1), '% the total ammount donated.')

    #%% [markdown]
    # ## How much the companies donated?

    #%%
    g_companies        = candidatos.groupby('cat_original_donator_name2').agg({'id_candidate_cpf': lambda x: x.nunique(),'id_election': lambda x: len(x),'num_donation_ammount':lambda x: [x.sum(), x.mean(), x.std()] })
    tab_companies      = g_companies.sort_values(by='num_donation_ammount', ascending=False).reset_index()

    #%%
    tab_companies["Total Ammont"] = tab_companies['num_donation_ammount'].apply(lambda x: x[0])
    tab_companies["Mean Ammount"] = tab_companies['num_donation_ammount'].apply(lambda x: x[1])
    tab_companies["Std  Ammount "]  = tab_companies['num_donation_ammount'].apply(lambda x: x[2])
    tab_companies.drop('num_donation_ammount', axis=1, inplace= True)


    #%%
    tab_companies.columns = ['Company Name', 'Number of Candidates', 'Number of Donations', 'Total (R$)','Mean (R$)', 'Standard Deviation (R$)' ]


    #%%
    print("Donation Statistics by Company\n",tab_companies[tab_companies['Number of Candidates'] > 0.01*n_candidatos])
    with open(table_path / f'brazil_2014_{role}_statistics_by_company.tex', 'w') as tf:
        tf.write(tab_companies[tab_companies['Number of Candidates'] > 0.01*n_candidatos].to_latex(index=False))


    #%%
    print('The 3 companies that most donated to candidates are:')
    [print(i) for i in tab_companies['Company Name'][:3].values]


    #%%
    print('Alone, they are reponsable for', round(tab_companies['Number of Donations'][:3].sum()/n_donations*100,1), '% of all donations.')


    #%%
    print('Corresponding to',  round(tab_companies['Total (R$)'][:3].sum()/total_donations*100,1), '% the total ammount donated.')


    #%%
    n_companies = len(tab_companies)


    #%%
    print('This 3 companies correspond to',  round(3/n_companies*100,3), '% the companies who donated.')


    #%%
    top_3_companies = ['JBS S/A',
                        'CONSTRUTORA QUEIROZ GALVAO S A',
                        'U T C ENGENHARIA S/A']


    #%%
    donator_top3_companies = candidatos[candidatos['cat_original_donator_name2'].isin(top_3_companies)]
    g_parties        = donator_top3_companies.groupby('cat_party').agg({'cat_original_donator_name2': lambda x: x.nunique(),'id_election': lambda x: len(x),'num_donation_ammount':lambda x: [x.sum(), x.mean(), x.std()] })
    tab_parties      = g_parties.sort_values(by='num_donation_ammount', ascending=False).reset_index()


    #%%
    tab_parties["Total Ammont"] = tab_parties['num_donation_ammount'].apply(lambda x: x[0])
    tab_parties["Mean Ammount"] = tab_parties['num_donation_ammount'].apply(lambda x: x[1])
    tab_parties["Std  Ammount "]  = tab_parties['num_donation_ammount'].apply(lambda x: x[2])
    tab_parties.drop('num_donation_ammount', axis=1, inplace= True)
    tab_parties.reset_index()

    #%%
    tab_parties.columns = ['Party', 'Number of Companies', 'Number of Donations', 'Total (R$)','Mean (R$)', 'Standard Deviation (R$)' ]


    #%%
    print("Donation Statistics by Party\n",tab_parties)
    with open(table_path / f'brazil_2014_{role}_statistics_by_party.tex', 'w') as tf:
        tf.write(tab_parties.to_latex(index=False))


    #%%
    print('The 3 parties most benefited from top 3 companies:')
    [print(i) for i in tab_parties['Party'][:3].values]


    #%%
    n_donations = tab_parties['Number of Donations'].sum()
    total_donations = tab_parties['Total (R$)'].sum()


    #%%
    print('Alone, they are reponsable for', round(tab_parties['Number of Donations'][:3].sum()/n_donations*100,2), '% of all donations.')


    #%%
    print('Corresponding to',  round(tab_parties['Total (R$)'][:3].sum()/total_donations*100,2), '% the total ammount donated.')


    #%% [markdown]
    # -----
    #%% [markdown]
    # ## How many donation each candidate received?

    #%%
    g_candidates       = candidatos.groupby('id_candidate_cpf').agg({'cat_original_donator_name2': lambda x: x.nunique(),'id_election': lambda x: len(x),'num_donation_ammount':lambda x: [x.sum(), x.mean(), x.std()] })
    tab_candidates      = g_candidates.sort_values(by='num_donation_ammount', ascending=False).reset_index()


    #%%
    tab_candidates["Total Ammont"] = tab_candidates['num_donation_ammount'].apply(lambda x: x[0])
    tab_candidates["Mean Ammount"] = tab_candidates['num_donation_ammount'].apply(lambda x: x[1])
    tab_candidates["Std  Ammount "]  = tab_candidates['num_donation_ammount'].apply(lambda x: x[2])
    tab_candidates.drop('num_donation_ammount', axis=1, inplace= True)


    #%%
    tab_candidates.columns = ['Candidate CPF', 'Number of Companies', 'Number of Donations', 'Total (R$)','Mean (R$)', 'Standard Deviation (R$)' ]

    print("Donation Statistics by Candidate\n",tab_candidates)
    with open(table_path / f'brazil_2014_{role}_statistics_by_candidate.tex', 'w') as tf:
        tf.write(tab_candidates.to_latex(index=False))

    #%% [markdown]
    # # Parties donations' Network 
    # 
    #%% [markdown]
    # Adapting the approaches of \cite{Bursztyn2015a} and \cite{Zhang2008a}, the data was employed to create two networks: 
    # 
    # 
    # - A (bipartite) directed one where a legal entity is connected by an edge to each candidate it sponsored or cosponsored. This is encoded using a bipartite adjacency matrix $M$, with entries $M_{ij}$ equal to $1$ if legal entity $j$ donated to candidate $i$ and $0$ otherwise. 
    # 
    # - A second (unipartite), undirected one, projected from the first, with adjacency matrices $A_{ij} = \sum_k M_{ik} M^T_{kj}$ in which nodes are candidates and the weighted edges connecting them indicate how many times they received money together from the same legal entity $k$.ipynb_checkpoints/
    #%% [markdown]
    # The donnation network forms a bipartite network, a donor is connected by an edge to candidate it sponsored or cosponsored. This is encoded using a bipartite adjacency matrix M, with entries Mij equal to 1 if legal entity i (co-)sponsored candidate j and 0 if not.
    # We can analyze the cosponsorship networks using one-mode (“unipartite”) projections with adjacency matrices Aij, in which nodes are candidates and the weighted edges connecting them indicate how many times they shared donations from the same donors.
    #%% [markdown]
    # We generated the transactions network using the Python's library NetworkX\cite{NetworkX} and drew it using the interactive visualization and exploration platform Gephi\cite{Gephi}. 
    # To draw the graph we employed force-directed placement methods. We began giving more space to the graph and avoiding a randomized start using Fruchterman Reingold's algorithm \cite{Fruchterman}. After the system reached mechanical equilibrium, we applied Force Atlas 2 \cite{ForceAtlas2} to produce an aesthetically pleasant and intuitive graph.
    # To get a visual information about the importance of each node, its sizes was chosen to reflect the number of votes the corresponding candidate received. Finally, in order to detect patterns in donations we chose colors corresponding to communities extracted employing the Louvain Method for community detection \cite{Louvian}. 
#%% [markdown]
# ### Sao Paulo

#%%
deputados_estaduais_corruptos_sp = candidatos[(candidatos['id_donator_effective_cpf_cnpj'].isnull() == False)  
                                 & (candidatos['cat_election_state'] == "SP") ]

#%%
adj_sp = pd.DataFrame(0, index=deputados_estaduais_corruptos_sp.id_candidate_cpf.unique(), columns=deputados_estaduais_corruptos_sp.id_candidate_cpf.unique())
for donator in deputados_estaduais_corruptos_sp.id_donator_effective_cpf_cnpj.unique():
    rec = deputados_estaduais_corruptos_sp.loc[deputados_estaduais_corruptos_sp.id_donator_effective_cpf_cnpj == donator, 'id_candidate_cpf'].unique()
    cpf_pairs = list(itertools.combinations(rec,2))
    for cpf1, cpf2 in cpf_pairs:
        adj_sp.loc[cpf1, cpf2] += 1.0 
adj_sp.to_csv(data_path / 'deputados_estaduais_corruptos_sp__adj_matrix.csv')       

#%%
import igraph
# Get the values as np.array, it's more convenenient.
A = adj_sp.values
# Create graph, A.astype(bool).tolist() or (A / A).tolist() can also be used.
G_SP = igraph.Graph.Adjacency((A > 0).tolist())
# Add edge weights and node labels.
G_SP.es['weight'] = A[A.nonzero()]
G_SP.vs['label'] = adj_sp.index  # or a.index/a.columns

part_sp = louvain.find_partition(G_SP, louvain.ModularityVertexPartition, weights='weight')
p_summary = part_sp.summary()
p_quality = part_sp.quality()
p_modularity = part_sp.modularity
g_n_vertices =  G_SP.vcount()
g_n_edges    =  G_SP.ecount()
g_avg_degree = np.mean(G_SP.degree())


G_SP.vs['community'] = part_sp.membership
#%%
#Simple check
df_from_g = pd.DataFrame(G_SP.get_adjacency(attribute='weight').data,
                         columns=G_SP.vs['label'], index=G_SP.vs['label'])
(df_from_g == adj_sp).all().all() 
#%%
print('Partitions Summary:', p_summary)
print('Quality: ', p_quality)
print('Modularity: ', p_modularity)
print('Number of vertices (nodes):', g_n_vertices)
print('Number of edges:', g_n_edges)
print('Average Degree:', g_avg_degree)

pd.DataFrame.from_dict({'Partitions Summary': [p_summary], 
              'Quality': [p_quality],
              'Modularity':[p_modularity]}
              ).to_csv(data_path / 'deputados_estaduais_corruptos_sp__partition_summary.csv')

pd.DataFrame.from_dict({'Number of vertices (nodes)':[g_n_vertices], 
              'Number of edges':[g_n_edges],
              'Average Degree': [g_avg_degree]}
              ).to_csv(data_path / 'deputados_estaduais_corruptos_sp__network_summary.csv')

#%% [markdown]
# ## Community Detection
#%% [markdown]
# A community is based in the intuitive fact that a given group should have nodes more densely connected internally than with the rest of the network other groups, and measured by its modularity $Q$, defined as the fraction of the edge weight contained within the specified communities minus the expected total weight of such edges:
# 
# $Q = \frac{1}{2m}\sum_{ij} \left[A_{ij} - \frac{k_ik_j}{2m}\right] \delta(g_i, g_j)$
# 
# where $m$ is the total weight of the edges in the network, $k_i$ is the (weighted) degree of the i-th node, $g_i$ is the community to which $i$ belongs, and $\delta(g_i, g_j) = 1$ if $i$ and $j$ belong to the same community and $0$ otherwise.
# 
# Because modularity measures the extent to which the identified interactions between candidates take place within the identified community partitions rather than across them, it can be used to quantify the relationship between a group of legal entities with a group of candidates. This is an important because modularity does not depend on the assumption that the parties themselves are the relevant communities.  


#%%
pr = dict(zip(G_SP.vs['label'],G_SP.pagerank(weights='weight')))
maxPR = max(pr.values())

flip_PR = dict(zip(sorted(dict(zip(G_SP.vs['label'],G_SP.pagerank(weights='weight'))), key=lambda node: pr[node], reverse=True), range(0, G_SP.vcount()) )) 

pr_seq_sp=sorted(pr,reverse=True) # degree sequence


#%%
## VERIFICAR A LEI DE BENFORD PARA CANDIDATOS A ESQUERDA DO A DIREITA


#%%
# Run louvain community finding algorithm
louvain_community_dict = dict(zip(G_SP.vs['label'], G_SP.vs['community']))

# Convert community assignmet dict into list of communities
louvain_comms = defaultdict(list)
for node_index, comm_id in louvain_community_dict.items():
    louvain_comms[comm_id].append(node_index)
louvain_comms = louvain_comms.values()

nodes_louvain_ordered = [node for comm in louvain_comms for node in comm]


#%%
from operator import itemgetter
nodes_ordered=[]
for comm in louvain_comms:
    nodePR = []
    for node in comm:
        nodePR.append((node, pr[node]))
   
    nodePR =sorted(nodePR, key=itemgetter(1))
    nodes_ordered.append(nodePR)
    


#%%
nodes_list = []
for sublist in nodes_ordered:
    for item in sublist:
        nodes_list.append(item[0])


#%%
from random import randint
cores = []

for i in range(len(louvain_comms)):
    cores.append('%06X' % randint(0, 0xFFFFFF))
    #cores.append('blue')

#%%
draw_adjacency_matrix(adj_sp, nodes_list, louvain_comms , colors=cores, output_file = figure_path / 'deputados_estaduais_corrupts_sp__adj_matrix.pgf')

#%% [markdown]
# # Benford Law
#%% [markdown]
# The null hypothesis is that the data follow the Benford distribution. The test statistic follows a $\chi^2$ distribution, so the null hypothesis is rejected if $\chi^2 > \chi^2_{\alpha,8}$ where $\alpha$ is the level of significance \cite{TamCho2007}.
#%% [markdown]
# ## Benford`s Laws in Donations by Party
#%% [markdown]
# The null hypothesis is that the data follow the Benford distribution. The test statistic follows a $\chi^2$ distribution, so the null hypothesis is rejected if $\chi^2 > \chi^2_{\alpha,8}$ where $\alpha$ is the level of significance \cite{TamCho2007}.

#%%
benford_lv = bf.benford_digits_table(candidatos, 'cat_party')
benford_lv

#%% [markdown]
# ## Benford`s Laws in Donations by State
#%% [markdown]
# The null hypothesis is that the data follow the Benford distribution. The test statistic follows a $\chi^2$ distribution, so the null hypothesis is rejected if $\chi^2 > \chi^2_{\alpha,8}$ where $\alpha$ is the level of significance \cite{TamCho2007}.

#%% [markdown]
# ## Benford`s Laws in Donations by Economic sector
#%% [markdown]
# The null hypothesis is that the data follow the Benford distribution. The test statistic follows a $\chi^2$ distribution, so the null hypothesis is rejected if $\chi^2 > \chi^2_{\alpha,8}$ where $\alpha$ is the level of significance \cite{TamCho2007}.

#%% [markdown]
# ## Sao Paulo

#%%
deputados_estaduais_corruptos_sp.loc[:,"lv_community"] = deputados_estaduais_corruptos_sp.loc[:,"id_candidate_cpf"].map(louvain_community_dict)
deputados_estaduais_corruptos_sp[["id_candidate_cpf","lv_community"]].to_csv(data_path / 'deputados_estaduais_corruptos_sp__communities.csv')


#%%

all_digits = bf.read_numbers(deputados_estaduais_corruptos_sp.num_donation_ammount)       
all_probs = bf.find_probabilities(all_digits)

##all_digits_model = bf.read_numbers(df.num_donation_ammount)       
##all_probs_model = bf.find_probabilities(all_digits_model)

x2 = bf.find_x2(pd.Series(all_digits))[0]
#x2_model = bf.find_x2(pd.Series(all_digits_model))[0]

width = 0.2

indx = np.arange(1, 10)
benford = [np.log10(1 + (1.0 / d)) for d in indx]

plt.bar(indx, benford,width, color='#1DACD6', label="Lei de Benford",)
plt.bar(indx+width , all_probs, width, color='r', label=r'Doações($\chi^2$''='+str(round(x2,2))+')')
##plt.bar(indx+2*width , all_probs_model, width, color='#4d4d4d', label=r'Modelo($\chi^2$''='+str(round(x2_model,2))+')')
#plt.yscale('log')
#plt.xscale('log')
ax = plt.gca()
ax.set_xticks(indx)
ax.set_yticks([0.05,0.1, 0.2, 0.4])
ax.get_xaxis().set_major_formatter(matplotlib.ticker.ScalarFormatter())
ax.get_yaxis().set_major_formatter(matplotlib.ticker.ScalarFormatter())
plt.title("Comm 0")
plt.ylabel("Probabilidade")
plt.grid(False)
plt.legend()
plt.tight_layout()


#%%
N_doa = list(deputados_estaduais_corruptos_sp.groupby('lv_community').agg({"num_donation_ammount": 'count'}).to_dict().values())[0]


#%%
comunity_receita = deputados_estaduais_corruptos_sp.groupby('lv_community').agg({"num_donation_ammount": 'sum'}).reset_index()


#%%
benford_lv = bf.benford_digits_table(deputados_estaduais_corruptos_sp, 'lv_community')
benford_lv.to_csv(data_path / 'deputados_estaduais_corruptos_sp__benford_law.csv')
benford_lv_200 = benford_lv[benford_lv['N'] >= 200]
print("Lei de Benford aplicada as doações:\n",benford_lv_200)
with open(table_path / 'deputados_estaduais_corruptos_sp__benford_law__gt_200.tex', 'w') as tf:
     tf.write(benford_lv_200.reset_index().rename(columns={'index':'Community'}).to_latex(index=False))


#%% [markdown]
# ### Communities donations by generative model
#%% [markdown]
# For each one of the n communities found, each containing m candidates, we fit a generative model with the data of each donation made whithin a given community to a given candidate. The amount of donation i in community j = {1, ..., n} to a candidade k, is determined by a random variable having the distribution generate by our artificial model.

#%%
df = deputados_estaduais_corruptos_sp[['lv_community','id_candidate_cpf','num_donation_ammount']].copy()
n_donations = deputados_estaduais_corruptos_sp.groupby(['lv_community','id_candidate_cpf']).agg({'num_donation_ammount': lambda x: sum(x)})
comus = list(n_donations.index.get_level_values('lv_community').unique())



#%%
with open(data_path / 'deputados_estaduais_corruptos_sp__gm_leiden_parameters.csv','w') as f1:
    writer=csv.writer(f1, delimiter=',',lineterminator='\n') 
    header = ['xmin', 'xmax', 'gamma', 'eta0']
    writer.writerow(header)
    for comu in comus:
        cpfs = list(df.loc[df.lv_community == comu, 'id_candidate_cpf'].unique())
        X    = df.loc[df.lv_community == comu, 'num_donation_ammount']
        xmin = min(X.values)
        xmax = max(X.values)
        result = generative_model.maxLKHD_distr(X.values, gamma=1., csi0=1, csim=np.log(xmax), delt=np.log(0.005))
        gamma = result[0]
        eta0 = result[1]
        row = [xmin, xmax, gamma, eta0]
        writer.writerow(row)
        print('Comunidade: ',comu)
        print('---------------------')
        print('xmin:' ,xmin, 'xmax', xmax)
        print('Gamma:' ,gamma, 'Eta_0', eta0)
        print('---------------------')
        for cpf in cpfs :
            idx = df.loc[(df['lv_community'] == comu) 
                                & (df['id_candidate_cpf'] == cpf), 'num_donation_ammount'].index
            for _idx in idx:
                df.loc[_idx, 'num_donation_ammount'] = generative_model.drand_distr(gamma, eta0, csim=np.log(xmax), delt=np.log(0.005))

 
#%%
df.to_csv(data_path / 'deputados_estaduais_corruptos_sp__gm_leiden_input.csv', index=False)
df = pd.read_csv(data_path / 'deputados_estaduais_corruptos_sp__gm_leiden_input.csv')
#%%
parameters = pd.read_csv(data_path / 'deputados_estaduais_corruptos_sp__gm_leiden_parameters.csv')
parameters_200 = parameters.loc[benford_lv_200.index]
print('Model Parameters', parameters_200)
with open(table_path / 'deputados_estaduais_corruptos_sp__benford_law__gt_200__gm_parameters.tex', 'w') as tf:
     tf.write(parameters_200.reset_index().rename(columns={'index':'Community'}).to_latex(index=False))
#%%
benford_lv_gen = bf.benford_digits_table(df, 'lv_community')
benford_lv_gen_200  = benford_lv_gen.loc[benford_lv_200.index]
print('Benford Law Results expected by the model', benford_lv_gen_200)
with open(table_path / 'deputados_estaduais_corruptos_sp__benford_law__gt_200__gm.tex', 'w') as tf:
     tf.write(benford_lv_gen_200.reset_index().rename(columns={'index':'Community'}).to_latex(index=False))


#%%
# We can ask for ALL THE AXES and put them into axes
fig, axes = plt.subplots(nrows=4, ncols=3, sharex=True, sharey=True, figsize=(18,32))
axes_list = [item for sublist in axes for item in sublist] 

for com in benford_lv_200.index:
    ax = axes_list.pop(0)
    H2,X2 = np.histogram( np.log(deputados_estaduais_corruptos_sp.loc[deputados_estaduais_corruptos_sp['lv_community'] == com,'num_donation_ammount']), density=True )
    dx2 = X2[1] - X2[0]
    F2 = np.cumsum(H2)*dx2
    ax.plot(X2[1:], F2, label='data')
    ax.fill_between(X2[1:], F2, alpha=0.2)
    
    H2,X2 = np.histogram( np.log(df.loc[df['lv_community'] == com,'num_donation_ammount']), density=True )
    dx2 = X2[1] - X2[0]
    F2 = np.cumsum(H2)*dx2
    ax.plot(X2[1:], F2, color = 'r', label='model')
 
    ax.legend()
    ax.set_title('Community Index: '+ str(com))
    ax.set_xlabel("")
    ax.tick_params(
        which='both',
        bottom=False,
        left=False,
        right=False,
        top=False
    )
    ax.grid(linewidth=0.25)
    ax.set_xlabel('$\ln(x)$')
    ax.set_ylabel('$F(x)$')
    ax.set_xlim((-2, 12))
    ax.set_xticks((-2,0,2, 4, 6, 8,10, 12))
    ax.spines['left'].set_visible(False)
    ax.spines['top'].set_visible(False)
    ax.spines['right'].set_visible(False)



for ax in axes_list:
    ax.remove()

plt.tight_layout()
plt.subplots_adjust(hspace=0.3)

plt.savefig(figure_path / 'deputados_estaduais_corruptos_sp__community_leiden_cdf.pgf')
plt.show()


#%%
benf_out = benford_lv_200[benford_lv_200['d'] > 1.569]
benf_in = benford_lv_gen_200[benford_lv_gen_200['d'] <= 1.569]


#%%
benf_divergence = benf_out.reindex(benf_in.index).dropna().index


#%%
benford_lv_gen_200


#%%
benf_out.loc[benf_divergence]


#%% [markdown]
# ### Comparing with CNEP and CEIS data.
# - http://www.portaltransparencia.gov.br/download-de-dados/cnep
# - http://www.portaltransparencia.gov.br/download-de-dados/ceis
CNEP = pd.read_csv(data_path / '20190301_CNEP.csv')
CEIS = pd.read_csv(data_path / '20190301_CEIS.csv')

#%%
cnpjs_sujos = CNEP['CPF OU CNPJ DO SANCIONADO'].append(
              CEIS['CPF OU CNPJ DO SANCIONADO']).apply(lambda x: str(x).zfill(14)) #< contem cpfs tambem


#%%
print('Numero de cnpjs sujos: {}'.format( cnpjs_sujos.nunique()))


#%%
deputados_estaduais_corruptos_sp.loc[:,'id_donator_effective_cpf_cnpj_str'] = deputados_estaduais_corruptos_sp.id_donator_effective_cpf_cnpj.apply(lambda x: str(int(x)).zfill(14))


#%%
deputados_estaduais_corruptos_sp.loc[:,'sujos'] = deputados_estaduais_corruptos_sp['id_donator_effective_cpf_cnpj_str'].isin(cnpjs_sujos)


#%%
doadores_sujos = deputados_estaduais_corruptos_sp.loc[deputados_estaduais_corruptos_sp['sujos'], 'id_donator_effective_cpf_cnpj_str'].unique()
n_doadores_sujos = deputados_estaduais_corruptos_sp.loc[deputados_estaduais_corruptos_sp['sujos'], 'id_donator_effective_cpf_cnpj_str'].nunique()

#%%
print('Numero de doacoes de cpfs/cnpjs sujos: {}'.format(deputados_estaduais_corruptos_sp['sujos'].sum()))


#%%
print('Numero de cpfs/cnpjs sujos que doaram: {}'.format(n_doadores_sujos))

#%%
#contagem de doacoes sujas nas comunidades
comunidades_sujos = deputados_estaduais_corruptos_sp.groupby('lv_community').agg({'sujos': lambda x: sum(x)}).sort_values(by='sujos', ascending=False)
comunidades_sujos.columns = ['# Dirty Donations']
benford_lv = benford_lv.join(comunidades_sujos)
#benford_lv = benford_lv.join(comunidades_sujos)

#%%
#numero de doadores na comunidade
comunidades_doadores = deputados_estaduais_corruptos_sp.groupby('lv_community').agg({'id_donator_effective_cpf_cnpj': lambda x: x.nunique()})
comunidades_doadores.columns = ['# Donors']
benford_lv = benford_lv.join(comunidades_doadores)

#%%
#numero de doares sujos na comunidade
comunidades_doadores_sujos = deputados_estaduais_corruptos_sp.loc[deputados_estaduais_corruptos_sp.id_donator_effective_cpf_cnpj.isin(doadores_sujos)].groupby('lv_community').agg({'id_donator_effective_cpf_cnpj':lambda x: int(x.nunique())})
comunidades_doadores_sujos.columns = ['# Dirty Donors']
benford_lv = benford_lv.join(comunidades_doadores_sujos)

#%%
#total sujo doado na comunidade
comunidades_total_sujo = deputados_estaduais_corruptos_sp.loc[deputados_estaduais_corruptos_sp.id_donator_effective_cpf_cnpj.isin(doadores_sujos)].groupby('lv_community').agg({'num_donation_ammount':lambda x: sum(x)})
comunidades_total_sujo.columns = ['Total Dirty Amount']
benford_lv = benford_lv.join(comunidades_total_sujo)


#%%
#Soma das doacoes na comunidade:
comunidades_total_doacoes = deputados_estaduais_corruptos_sp.groupby('lv_community').agg({'num_donation_ammount':sum})
comunidades_total_doacoes.columns = ['Total Amount']
benford_lv = benford_lv.join(comunidades_total_doacoes)
#%%
#Soma das doacoes na comunidade:
comunidades_numero_candidatos = deputados_estaduais_corruptos_sp.groupby('lv_community').agg({'num_donation_ammount':count})
comunidades_numero_candidatos.columns = ['Number of Candidates']
benford_lv = benford_lv.join(comunidades_numero_candidatos)

#%%
#Parametros da comunidade:
benford_lv = benford_lv.join(parameters)

#%%
print('Benford Law Results with dirty measure:', benford_lv)
benford_lv.to_csv(data_path / 'deputados_estaduais_corruptos_sp__benford_law_dirty_donations.csv')
with open(table_path / 'deputados_estaduais_corruptos_sp__benford_law_dirty_donations.tex', 'w') as tf:
     tf.write(benford_lv.reset_index().rename(columns={'index':'Community'}).to_latex(index=False))


#%%
print('Benford Law Results with dirty measure gt 200:', benford_lv[benford_lv_200.index])
benford_lv.to_csv(data_path / 'deputados_estaduais_corruptos_sp__benford_law_dirty_donations__gt_200.csv')
with open(table_path / 'deputados_estaduais_corruptos_sp__benford_law_dirty_donations__gt_200.tex', 'w') as tf:
     tf.write( benford_lv[benford_lv_200.index].reset_index().rename(columns={'index':'Community'}).to_latex(index=False))